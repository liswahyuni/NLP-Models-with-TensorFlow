# -*- coding: utf-8 -*-
"""Proyek 1 Pengembangan ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZTrJHnVKS85UV4AtuAsnhshbxDO_yc2G

Nama: Lis Wahyuni

Download dataset from Kaggle
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle

!cp /content/kaggle.json ~/.kaggle/

# Permission for the json to act
!chmod 600 ~/.kaggle/kaggle.json

# List all datasets in Kaggle
!kaggle datasets list

!kaggle datasets download -d rahulanand0070/youtubevideodataset

!ls

!unzip youtubevideodataset.zip

import pandas as pd

df = pd.read_csv('Youtube Video Dataset.csv')
df

df.info()

"""# **Specify the "Category" using "Description" of video**"""

# Proses one-hot-encoding
dum_cate = pd.get_dummies(df.Category)
df_baru = pd.concat([df, dum_cate], axis=1)
df_baru = df_baru.drop(columns='Category')
df_baru

df_baru.drop(columns=['Title', 'Videourl'])

df_baru.Description=df_baru.Description.astype(str)

import numpy as np

# Ubah nilai-nilai dari df_baru ke tipe data numpy array
desc_values = df_baru['Description'].values
label = df_baru[['Art&Music', 'Food', 'History', 'Science&Technology', 'manufacturing', 'travel blog']].values

# split data jadi training-testing
from sklearn.model_selection import train_test_split
desc_values_train, desc_values_test, label_train, label_test = train_test_split(desc_values, label, test_size=0.2)

# Ubah tiap kata di dataset ke bilangan numerik menggunakan fungsi Tokenizer
# Lalu konversi tiap sample jadi sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#YouTube gives us 5,000 characters (about 800 words) for the description 
tokenizer = Tokenizer(num_words=5000, oov_token='x') 
tokenizer.fit_on_texts(desc_values_train) 
tokenizer.fit_on_texts(desc_values_test)
	 
sekuens_train = tokenizer.texts_to_sequences(desc_values_train)
sekuens_test = tokenizer.texts_to_sequences(desc_values_test)
	 
padded_train = pad_sequences(sekuens_train) 
padded_test = pad_sequences(sekuens_test)

# Arsitektur model pakai layer Embedding, dimensi= 16, & dimensi dari input=num_words
import tensorflow as tf
from tensorflow import keras

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(0.001),metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah di atas 90%.")
      self.model.stop_training = True
callbacks = myCallback()

# Train model
result = model.fit(padded_train, label_train, epochs=100, 
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt

plt.figure(figsize=[10,8])
plt.plot(result.history['loss'], 'black', linewidth=2.0)
plt.plot(result.history['val_loss'], 'red', linewidth=2.0)
plt.legend(['Training Loss', 'Validation Loss'], fontsize=14, loc='best')
plt.title('Loss Curves', fontsize=12)
plt.ylabel('Loss', fontsize=10)
plt.xlabel('Epochs', fontsize=10)
plt.show()

plt.figure(figsize=[10,8])
plt.plot(result.history['accuracy'], 'blue', linewidth=2.0)
plt.plot(result.history['val_accuracy'], 'orange', linewidth=2.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14, loc='best')
plt.title('Accuracy Curves', fontsize=12)
plt.ylabel('Accuracy', fontsize=10)
plt.xlabel('Epochs', fontsize=10)
plt.show()